---
phase: 17-import-connector-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/functions/_shared/connector-pipeline.ts
  - supabase/migrations/20260228000001_create_import_sources.sql
autonomous: true

must_haves:
  truths:
    - "checkDuplicate() correctly identifies existing recordings by source_app + external_id + owner_user_id"
    - "insertRecording() writes to recordings table with source_metadata containing external_id as first key"
    - "insertRecording() creates a vault_entry in the user's personal vault"
    - "import_sources table exists with user_id, source_app, is_active, last_sync_at, account_email columns"
    - "import_sources has RLS policies restricting users to their own rows"
  artifacts:
    - path: "supabase/functions/_shared/connector-pipeline.ts"
      provides: "Shared 5-stage connector pipeline utility"
      exports: ["ConnectorRecord", "PipelineResult", "checkDuplicate", "insertRecording", "runPipeline"]
    - path: "supabase/migrations/20260228000001_create_import_sources.sql"
      provides: "import_sources table with RLS"
      contains: "CREATE TABLE import_sources"
  key_links:
    - from: "supabase/functions/_shared/connector-pipeline.ts"
      to: "recordings table"
      via: "supabase.from('recordings').insert()"
      pattern: "from\\('recordings'\\)\\.insert"
    - from: "supabase/functions/_shared/connector-pipeline.ts"
      to: "vault_entries table"
      via: "supabase.from('vault_entries').insert()"
      pattern: "from\\('vault_entries'\\)\\.insert"
---

<objective>
Create the shared connector pipeline utility and the import_sources database table.

Purpose: The pipeline is the foundation for all connector work in this phase. Every connector (existing and new) will call checkDuplicate() and insertRecording() instead of writing their own DB insert logic. The import_sources table stores per-user, per-source connection state for the Import Hub UI.

Output: `_shared/connector-pipeline.ts` with exported functions + `import_sources` migration deployed.
</objective>

<execution_context>
@/Users/Naegele/.claude/get-shit-done/workflows/execute-plan.md
@/Users/Naegele/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-import-connector-pipeline/17-RESEARCH.md
@.planning/phases/15-data-migration/15-01-SUMMARY.md
@.planning/phases/15-data-migration/15-03-SUMMARY.md
@supabase/functions/_shared/cors.ts
@supabase/migrations/20260131000007_create_recordings_tables.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared connector pipeline utility</name>
  <files>supabase/functions/_shared/connector-pipeline.ts</files>
  <action>
Create `supabase/functions/_shared/connector-pipeline.ts` with the following exports:

**Types:**
- `ConnectorRecord` interface: `{ external_id: string, source_app: string, title: string, full_transcript: string, recording_start_time: string, duration?: number, source_metadata: Record<string, unknown>, bank_id?: string, vault_id?: string }`
- `PipelineResult` interface: `{ success: boolean, recordingId?: string, skipped?: boolean, error?: string }`

**Stage 3 — Dedup check:**
- `checkDuplicate(supabase, userId, sourceApp, externalId)` → `{ isDuplicate: boolean, existingRecordingId?: string }`
- Query: `recordings` table, filter by `owner_user_id`, `source_app`, and `source_metadata->>'external_id'` using `.filter()` with `eq` operator
- Fail open on query error (log error, return `{ isDuplicate: false }`) — never block an import on a dedup check failure

**Stage 5 — Insert recording + vault entry:**
- `insertRecording(supabase, userId, record: ConnectorRecord)` → `{ id: string }`
- If `bank_id` not provided, resolve personal bank via `bank_memberships` JOIN `banks` where `type = 'personal'` and `user_id` matches
- Build `source_metadata` with `external_id` as first key: `{ external_id: record.external_id, ...record.source_metadata }`
- Insert into `recordings`: `bank_id`, `owner_user_id`, `title`, `full_transcript`, `source_app`, `source_metadata`, `duration`, `recording_start_time`, `global_tags: []`
- After recording insert, find personal vault: query `vaults` where `bank_id` matches and `vault_type = 'personal'`
- Insert `vault_entries` row: `{ vault_id, recording_id }` — wrap in try/catch, log but do NOT throw on vault_entry failure (per existing pattern from Phase 10 summary: "Vault entry creation non-blocking in edge functions")

**Convenience wrapper:**
- `runPipeline(supabase, userId, record: ConnectorRecord)` → `PipelineResult`
- Calls `checkDuplicate()`, returns `{ success: false, skipped: true }` if duplicate
- Calls `insertRecording()`, returns `{ success: true, recordingId }`
- Catches errors, returns `{ success: false, error: message }`

Use Deno-compatible imports: `import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'` for the SupabaseClient type (import type only). Follow the existing _shared module pattern from `cors.ts` and `fathom-client.ts`.

Do NOT use class-based design. Keep it as flat exported functions.
  </action>
  <verify>
- File exists at `supabase/functions/_shared/connector-pipeline.ts`
- Exports: `ConnectorRecord`, `PipelineResult`, `checkDuplicate`, `insertRecording`, `runPipeline`
- No class definitions — all flat async functions
- `checkDuplicate` filters by `owner_user_id`, `source_app`, AND `source_metadata->>'external_id'`
- `insertRecording` writes to both `recordings` and `vault_entries`
- `vault_entries` insert is wrapped in try/catch (non-blocking)
  </verify>
  <done>
connector-pipeline.ts exists with all 5 exports, uses flat function pattern, dedup checks all 3 fields, vault_entry insert is non-blocking
  </done>
</task>

<task type="auto">
  <name>Task 2: Create import_sources table migration</name>
  <files>supabase/migrations/20260228000001_create_import_sources.sql</files>
  <action>
Create migration `supabase/migrations/20260228000001_create_import_sources.sql`:

```sql
-- Import sources: tracks per-user connection status for each import source
CREATE TABLE IF NOT EXISTS import_sources (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  source_app TEXT NOT NULL,  -- 'fathom', 'zoom', 'youtube', 'file-upload'
  is_active BOOLEAN NOT NULL DEFAULT true,
  account_email TEXT,        -- connected account email (OAuth sources)
  last_sync_at TIMESTAMPTZ,
  error_message TEXT,        -- last error if any
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  UNIQUE(user_id, source_app)
);

-- RLS
ALTER TABLE import_sources ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their own import sources"
  ON import_sources FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert their own import sources"
  ON import_sources FOR INSERT
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update their own import sources"
  ON import_sources FOR UPDATE
  USING (auth.uid() = user_id)
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can delete their own import sources"
  ON import_sources FOR DELETE
  USING (auth.uid() = user_id);

-- Index for fast lookups
CREATE INDEX idx_import_sources_user_id ON import_sources(user_id);

-- Add skipped_count column to sync_jobs for dedup summary reporting
ALTER TABLE sync_jobs ADD COLUMN IF NOT EXISTS skipped_count INTEGER DEFAULT 0;

-- Function to get import counts per source (single query, all counts)
CREATE OR REPLACE FUNCTION get_import_counts(p_user_id UUID)
RETURNS TABLE(source_app TEXT, call_count BIGINT)
LANGUAGE sql STABLE SECURITY DEFINER
AS $$
  SELECT r.source_app, COUNT(*) as call_count
  FROM recordings r
  WHERE r.owner_user_id = p_user_id
  GROUP BY r.source_app;
$$;
```

Deploy to production via psql (same pattern as Phase 15):
```bash
PGHOST=aws-1-us-east-1.pooler.supabase.com PGPORT=5432 PGUSER=postgres.vltmrnjsubfzrgrtdqey PGPASSWORD=x2n2KlCAA8suZjqa PGDATABASE=postgres psql -f supabase/migrations/20260228000001_create_import_sources.sql
```

Verify table creation and RLS:
```sql
SELECT tablename FROM pg_tables WHERE tablename = 'import_sources';
SELECT policyname FROM pg_policies WHERE tablename = 'import_sources';
```
  </action>
  <verify>
- Migration file exists at `supabase/migrations/20260228000001_create_import_sources.sql`
- Table `import_sources` exists in production with columns: id, user_id, source_app, is_active, account_email, last_sync_at, error_message, created_at, updated_at
- UNIQUE constraint on (user_id, source_app)
- 4 RLS policies exist (SELECT, INSERT, UPDATE, DELETE)
- `get_import_counts()` function exists and returns source_app + count
- `sync_jobs` table has `skipped_count` column (INTEGER DEFAULT 0)
  </verify>
  <done>
import_sources table deployed to production with RLS, unique constraint on (user_id, source_app), and get_import_counts() RPC function
  </done>
</task>

</tasks>

<verification>
- `connector-pipeline.ts` exports are importable from other edge functions via `import { runPipeline } from '../_shared/connector-pipeline.ts'`
- `import_sources` table has RLS enabled with all 4 CRUD policies
- `get_import_counts()` returns accurate counts matching `SELECT source_app, COUNT(*) FROM recordings WHERE owner_user_id = $1 GROUP BY source_app`
</verification>

<success_criteria>
1. The shared pipeline utility exists and exports flat async functions for dedup + insert
2. The import_sources table exists in production with correct schema and RLS
3. No existing functionality is broken — this plan only adds new code
</success_criteria>

<output>
After completion, create `.planning/phases/17-import-connector-pipeline/17-01-SUMMARY.md`
</output>
