---
phase: 02-chat-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/functions/chat-stream-v2/index.ts
autonomous: true

must_haves:
  truths:
    - "streamText() with a tool() definition works on Deno via esm.sh"
    - "toUIMessageStreamResponse() returns valid SSE that the frontend useChat can consume"
    - "A single tool fires, executes, and returns results through the stream"
  artifacts:
    - path: "supabase/functions/chat-stream-v2/index.ts"
      provides: "New Edge Function skeleton with streamText + 1 tool"
      contains: "streamText"
  key_links:
    - from: "supabase/functions/chat-stream-v2/index.ts"
      to: "esm.sh/ai@5.0.102"
      via: "import streamText, tool"
      pattern: "streamText|tool"
    - from: "supabase/functions/chat-stream-v2/index.ts"
      to: "esm.sh/@openrouter/ai-sdk-provider"
      via: "import createOpenRouter"
      pattern: "createOpenRouter"
---

<objective>
Proof-of-concept: Verify that `streamText()` + `tool()` + `toUIMessageStreamResponse()` works on Deno/Supabase Edge Functions via esm.sh. This is the single highest-risk item in the entire phase — if this doesn't work, the fallback is manual SSE construction from `result.fullStream`.

Purpose: De-risk the SDK migration before investing in 14 tool definitions. Establish the working skeleton that all subsequent plans build on.
Output: A deployable `chat-stream-v2` Edge Function that accepts a chat message, calls a single tool (getCallDetails), streams the response back via AI SDK protocol.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-chat-foundation/02-RESEARCH.md
@.planning/phases/02-chat-foundation/02-CONTEXT.md

# Reference: Working AI SDK patterns on Deno
@supabase/functions/generate-ai-titles/index.ts
@supabase/functions/auto-tag-calls/index.ts

# Reference: Current chat-stream implementation (replicate auth, CORS, message conversion)
@supabase/functions/chat-stream/index.ts

# Reference: CORS helper
@supabase/functions/_shared/cors.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat-stream-v2 Edge Function skeleton with streamText + single tool</name>
  <files>supabase/functions/chat-stream-v2/index.ts</files>
  <action>
Create a NEW Edge Function `chat-stream-v2` that proves the AI SDK streaming + tool pattern works on Deno.

**Import pattern (proven in generate-ai-titles and auto-tag-calls):**
```typescript
import { streamText, tool, convertToModelMessages, stepCountIs } from 'https://esm.sh/ai@5.0.102';
import { createOpenRouter } from 'https://esm.sh/@openrouter/ai-sdk-provider@1.2.8';
import { z } from 'https://esm.sh/zod@3.23.8';
```

**Must include:**
1. **CORS handling** — Use `getCorsHeaders(origin)` from `../_shared/cors.ts`. Handle OPTIONS preflight.
2. **Auth** — Verify Supabase JWT from Authorization header. Create Supabase client with user's token. Extract `user.id` for session filtering.
3. **Request body parsing** — Accept `{ messages: UIMessage[], model?: string, filters?: object, sessionId?: string }`. Default model to `openai/gpt-4o-mini`.
4. **OpenRouter provider setup** — Use `createOpenRouter()` with apiKey from `Deno.env.get('OPENROUTER_API_KEY')`, headers `HTTP-Referer: https://app.callvaultai.com` and `X-Title: CallVault`.
5. **Message conversion** — Use `convertToModelMessages(messages)` to convert UIMessage[] to model format.
6. **ONE tool definition** — `getCallDetails` as a proof-of-concept tool:
   - `description`: "Get full details of a specific call/meeting by its recording ID"
   - `inputSchema`: `z.object({ recording_id: z.string().describe('The recording ID of the call') })`
   - `execute`: Query `fathom_calls` table via supabase for the given `recording_id` where `user_id` matches, returning call title, date, duration, speakers, summary, action items
7. **streamText() call** with:
   - `model: openrouter(selectedModel)`
   - `system`: A minimal system prompt: "You are CallVault AI, a helpful assistant that answers questions about the user's meeting transcripts and call recordings. Use the getCallDetails tool when the user asks about a specific call."
   - `messages: convertedMessages`
   - `tools: { getCallDetails }`
   - `toolChoice: 'auto'`
   - `stopWhen: stepCountIs(3)`
8. **Response** — `return result.toUIMessageStreamResponse({ headers: corsHeaders })`

**Fallback plan (document in code comment):** If `toUIMessageStreamResponse()` fails on Deno, use `result.fullStream` to manually construct SSE response with `ReadableStream` and `TextEncoder`. Add a commented-out fallback implementation.

**Do NOT include:** The other 13 tools, the full system prompt, the search pipeline, re-ranking, or embeddings. This is a minimal proof of concept.

**Error handling:** Wrap the entire handler in try/catch. On error, return JSON `{ error: message }` with appropriate status code and CORS headers.
  </action>
  <verify>
1. `supabase functions serve chat-stream-v2` starts without import errors
2. Send a test request with curl:
```bash
curl -X POST http://localhost:54321/functions/v1/chat-stream-v2 \
  -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello, what can you help me with?"}]}'
```
Verify: Response streams SSE events (text-delta events with content)
3. `tsc --noEmit` still passes (no regressions in existing code)
  </verify>
  <done>
- chat-stream-v2/index.ts exists and is a valid Deno Edge Function
- streamText() + tool() + toUIMessageStreamResponse() works on Deno (or fallback documented and implemented)
- Auth, CORS, and request parsing follow established patterns
- getCallDetails tool fires and returns data when invoked
- SSE stream contains proper AI SDK v5 Data Stream Protocol events
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify end-to-end streaming with frontend useChat</name>
  <files>supabase/functions/chat-stream-v2/index.ts</files>
  <action>
After Task 1 creates the skeleton, do a quick integration smoke test by temporarily pointing the frontend at chat-stream-v2.

1. Read `src/pages/Chat.tsx` to find the transport URL construction
2. Note the exact URL pattern: `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/chat-stream`
3. Temporarily modify Chat.tsx transport to use `chat-stream-v2` (just for testing — will revert)
4. Start the app with `npm run dev` and the edge function with `supabase functions serve`
5. Open the chat page, send a simple message like "What calls do I have?"
6. Verify:
   - Stream starts (text appears incrementally)
   - No console errors about protocol mismatch
   - `useChat` status transitions: streaming → ready
   - If the model calls `getCallDetails`, verify the tool call appears in the message parts
7. **Revert** the Chat.tsx transport URL change back to `chat-stream`

If the smoke test fails due to `toUIMessageStreamResponse()`:
- Check browser DevTools Network tab for the response format
- Implement the fallback using `result.fullStream` manual SSE
- Re-test

Document findings as a code comment at the top of chat-stream-v2/index.ts:
```typescript
// PoC Status: [PASS/FAIL]
// streamText + tool: [WORKS/NEEDS_FALLBACK]
// toUIMessageStreamResponse: [WORKS/MANUAL_SSE_NEEDED]
// Date: YYYY-MM-DD
```
  </action>
  <verify>
1. Frontend successfully receives and renders streamed text from chat-stream-v2
2. No protocol errors in browser console
3. Chat.tsx is reverted back to using `chat-stream` (original endpoint)
4. PoC status comment exists at top of chat-stream-v2/index.ts
  </verify>
  <done>
- End-to-end streaming verified: frontend useChat ↔ chat-stream-v2 ↔ OpenRouter
- Protocol compatibility confirmed (AI SDK v5 Data Stream Protocol)
- Any fallback needs documented and implemented
- Chat.tsx restored to original state (still pointing to chat-stream)
  </done>
</task>

</tasks>

<verification>
- `supabase functions serve chat-stream-v2` starts without errors
- SSE stream contains AI SDK protocol events (text-delta, tool-input-start, etc.)
- Frontend useChat can consume the stream without protocol errors
- Original /chat endpoint unaffected (chat-stream still serving)
- `tsc --noEmit` passes clean
</verification>

<success_criteria>
- chat-stream-v2 Edge Function exists and streams responses via AI SDK
- Proof-of-concept validated: streamText + tool + toUIMessageStreamResponse works on Deno
- Or: fallback implemented and documented if toUIMessageStreamResponse doesn't work
- Zero regressions to existing chat functionality
</success_criteria>

<output>
After completion, create `.planning/phases/02-chat-foundation/02-01-SUMMARY.md`
</output>
