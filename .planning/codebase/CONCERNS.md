# CallVault Technical Concerns & Recommendations

**Analysis Date:** 2026-02-09  
**Scope:** Performance, scalability, maintainability, technical debt

---

## Executive Summary

**Overall Health: GOOD with notable concerns**

The CallVault chat backend is well-architected for current usage but has scalability bottlenecks that will become critical as user count grows. The primary concerns are synchronous processing, lack of caching, and monolithic code organization.

**Priority Matrix:**

| Priority | Concern | Impact | Effort |
|----------|---------|--------|--------|
| ðŸ”´ HIGH | No result caching | Performance | Medium |
| ðŸ”´ HIGH | Blocking re-ranking | Latency | Medium |
| ðŸŸ¡ MEDIUM | Monolithic edge function | Maintainability | High |
| ðŸŸ¡ MEDIUM | No query embedding cache | Cost/Perf | Low |
| ðŸŸ¢ LOW | Hardcoded pricing | Maintenance | Low |

---

## 1. High Priority Concerns

### 1.1 No Result Caching (CRITICAL SCALABILITY ISSUE)

**Problem:**
Every chat request re-executes the full RAG pipeline:
1. Generate query embedding (200-500ms)
2. Run hybrid search RPC (50-200ms)
3. Re-rank results (300-800ms)
4. Lookup share URLs (20-50ms)

**Total: 600-1500ms per tool call**

**When LLM calls 3-5 tools in parallel (common for complex queries):**
- Sequential execution: 1.8-7.5 seconds
- No caching of identical sub-queries

**Impact:**
- Poor user experience (high latency)
- Unnecessary API costs (repeated embeddings)
- Rate limit exhaustion under load
- Cannot scale to 1000+ concurrent users

**Files Affected:**
- `supabase/functions/_shared/search-pipeline.ts` (lines 75-389)
- `supabase/functions/_shared/embeddings.ts` (lines 18-44)

**Recommended Fix:**

```typescript
// Add to _shared/search-pipeline.ts
import { Redis } from 'https://deno.land/x/upstash_redis/mod.ts';

const redis = new Redis({
  url: Deno.env.get('UPSTASH_REDIS_REST_URL'),
  token: Deno.env.get('UPSTASH_REDIS_REST_TOKEN'),
});

const CACHE_TTL_SECONDS = 300; // 5 minutes

async function getCachedSearch(query: string, filters: SearchFilters): Promise<HybridSearchResponse | null> {
  const cacheKey = `search:${hashQuery(query, filters)}`;
  const cached = await redis.get(cacheKey);
  return cached ? JSON.parse(cached) : null;
}

async function cacheSearchResults(query: string, filters: SearchFilters, results: HybridSearchResponse): Promise<void> {
  const cacheKey = `search:${hashQuery(query, filters)}`;
  await redis.setex(cacheKey, CACHE_TTL_SECONDS, JSON.stringify(results));
}
```

**Estimated Impact:**
- 60-80% latency reduction for repeated queries
- 50-70% embedding API cost reduction
- Enables 5-10x more concurrent users

---

### 1.2 Blocking Re-ranking Serializes Tool Calls

**Problem:**
The cross-encoder re-ranking is synchronous and blocks the entire search pipeline:

```typescript
// From search-pipeline.ts lines 340-341
const reranked = await rerankResults(query, candidates, hfApiKey, limit * 2);
const diverse = diversityFilter(reranked, 2, limit);
```

**Issues:**
1. HuggingFace API has 1500ms timeout per batch
2. 30 candidates Ã— 1500ms = 45 seconds worst case
3. Sequential batch processing (not parallel)
4. No early termination if LLM already has enough context

**Impact:**
- High p99 latency (users experience occasional extreme slowness)
- Tool calls that should take 500ms take 2-5 seconds
- Poor user experience for complex queries

**Recommended Fix:**

```typescript
// Option 1: Parallel batch processing
async function rerankResultsParallel(
  query: string,
  candidates: SearchResult[],
  hfApiKey: string,
  topK: number
): Promise<SearchResult[]> {
  const batches = chunk(candidates.slice(0, RERANK_MAX_CANDIDATES), RERANK_BATCH_SIZE);
  
  // Process all batches in parallel
  const batchResults = await Promise.all(
    batches.map(batch => rerankBatch(query, batch, hfApiKey))
  );
  
  const scored = batchResults.flat();
  return scored.sort((a, b) => (b.rerank_score || 0) - (a.rerank_score || 0)).slice(0, topK);
}

// Option 2: Skip re-ranking for simple queries
async function executeHybridSearch(params: HybridSearchParams): Promise<...> {
  // ... search code ...
  
  // Skip re-ranking if query is simple (single word, no filters)
  const shouldRerank = query.split(' ').length > 2 || hasComplexFilters(filters);
  
  if (!shouldRerank) {
    return formatResults(candidates.slice(0, limit));
  }
  
  const reranked = await rerankResults(query, candidates, hfApiKey, limit * 2);
  // ...
}
```

**Estimated Impact:**
- 50-70% reduction in p99 latency
- Better resource utilization
- Improved user experience

---

### 1.3 Single Point of Failure: OpenAI Embeddings

**Problem:**
OpenAI is the ONLY embedding provider. If OpenAI API is down or rate-limited, search functionality is completely broken.

```typescript
// From _shared/embeddings.ts - no fallback
export async function generateQueryEmbedding(query: string, openaiApiKey: string): Promise<number[]> {
  const response = await fetch(OPENAI_EMBEDDINGS_URL, { ... });
  // No fallback if this fails
}
```

**Impact:**
- Complete search outage if OpenAI is unavailable
- No graceful degradation path
- Business continuity risk

**Recommended Fix:**

```typescript
// Multi-provider embedding strategy
const EMBEDDING_PROVIDERS = [
  { name: 'openai', generator: generateOpenAIEmbedding },
  { name: 'local', generator: generateLocalEmbedding }, // Fallback
];

export async function generateQueryEmbeddingWithFallback(query: string): Promise<number[]> {
  for (const provider of EMBEDDING_PROVIDERS) {
    try {
      return await provider.generator(query);
    } catch (error) {
      console.warn(`Embedding provider ${provider.name} failed:`, error);
      continue;
    }
  }
  throw new Error('All embedding providers failed');
}
```

**Alternative (Simpler):**
Implement query-level caching to reduce OpenAI dependency (see 1.1).

---

## 2. Medium Priority Concerns

### 2.1 Monolithic Edge Function

**Problem:**
`chat-stream-v2/index.ts` is 1410 lines with:
- 14 tool definitions
- Multiple helper functions
- System prompt builder
- All in one file

**Issues:**
- Difficult to test individual tools
- Merge conflicts likely with multiple developers
- Hard to navigate and understand
- No separation of concerns between HTTP handling and business logic

**Files Affected:**
- `supabase/functions/chat-stream-v2/index.ts` (1410 lines)

**Recommended Fix:**

```
supabase/functions/chat-stream-v2/
â”œâ”€â”€ index.ts              # HTTP handler, auth (200 lines)
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ index.ts          # Tool registry
â”‚   â”œâ”€â”€ search-tools.ts   # Tools 1-9 (search pipeline)
â”‚   â”œâ”€â”€ analytical-tools.ts # Tools 10-12
â”‚   â””â”€â”€ advanced-tools.ts   # Tools 13-14
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ prompts.ts        # System prompt builder
â”‚   â”œâ”€â”€ filters.ts        # Filter resolution
â”‚   â””â”€â”€ context.ts        # Business profile fetching
â””â”€â”€ types.ts              # Shared types
```

**Example Refactor:**

```typescript
// tools/search-tools.ts
import { tool } from 'https://esm.sh/ai@6.0.66';
import { z } from 'https://esm.sh/zod@3.23.8';

export function createSearchTools(deps: ToolDependencies) {
  return {
    searchTranscriptsByQuery: tool({
      description: 'General semantic and keyword search...',
      inputSchema: z.object({ ... }),
      execute: async ({ query, limit }) => {
        return deps.search('searchTranscriptsByQuery', query, limit);
      },
    }),
    // ... more search tools
  };
}
```

**Estimated Effort:** 2-3 days  
**Estimated Impact:** Improved maintainability, testability

---

### 2.2 No Embedding Cache at Query Level

**Problem:**
Identical queries generate identical embeddings every time:

```typescript
// Called every time a tool executes
const queryEmbedding = await generateQueryEmbedding(query, openaiApiKey);
```

**Impact:**
- Wasteful API calls
- Increased latency
- Higher costs

**Recommended Fix:**

```typescript
// Add to _shared/embeddings.ts
const embeddingCache = new Map<string, { embedding: number[]; timestamp: number }>();
const CACHE_TTL_MS = 5 * 60 * 1000; // 5 minutes

export async function generateQueryEmbeddingCached(query: string, openaiApiKey: string): Promise<number[]> {
  const cacheKey = hashString(query.toLowerCase().trim());
  const cached = embeddingCache.get(cacheKey);
  
  if (cached && Date.now() - cached.timestamp < CACHE_TTL_MS) {
    console.log('[embeddings] Cache hit for query');
    return cached.embedding;
  }
  
  const embedding = await generateQueryEmbedding(query, openaiApiKey);
  embeddingCache.set(cacheKey, { embedding, timestamp: Date.now() });
  return embedding;
}
```

**Note:** In-memory cache only works within single function invocation. For cross-request caching, use Redis.

---

### 2.3 Hardcoded Pricing Requires Code Changes

**Problem:**
Model pricing is hardcoded in `_shared/usage-tracker.ts`:

```typescript
const PRICING: Record<string, { input: number; output: number }> = {
  'openai/gpt-4o-mini': { input: 0.15, output: 0.60 },
  'anthropic/claude-3-opus': { input: 15.00, output: 75.00 },
  // ... 30+ models
};
```

**Issues:**
- Adding new models requires code deployment
- Price changes need code updates
- No validation that pricing matches actual OpenRouter costs

**Recommended Fix:**

```typescript
// Fetch pricing from database (synced from OpenRouter API)
async function getPricing(supabase: SupabaseClient): Promise<Record<string, Pricing>> {
  const { data } = await supabase.from('model_pricing').select('model_id, input_price, output_price');
  return Object.fromEntries(data.map(p => [p.model_id, { input: p.input_price, output: p.output_price }]));
}
```

**Sync Strategy:**
- Background job fetches OpenRouter pricing daily
- Updates `model_pricing` table
- Edge function reads from table

---

### 2.4 Large File Size May Impact Cold Start

**Problem:**
`chat-stream-v2/index.ts` is 1410 lines. While Deno/Supabase edge functions handle this, larger functions have:
- Slower cold starts
- Higher memory usage
- Longer deployment times

**Current Size:**
```bash
$ wc -l supabase/functions/chat-stream-v2/index.ts
1410 lines
```

**Recommended Fix:**
See 2.1 (modularization). Additionally:

```typescript
// Lazy load non-critical tools
const advancedTools = await import('./tools/advanced-tools.ts');
```

---

## 3. Low Priority Concerns

### 3.1 Limited Type Safety in Some Areas

**Problem:**
Several `any` types and `unknown` casts:

```typescript
// Line 552 in chat-stream-v2/index.ts
const diverse = diversityFilter(filtered as any[], 2, limit);

// Lines 619-620
const rpcParams: Record<string, unknown> = { ... }
```

**Impact:**
- Reduced IDE autocomplete
- Potential runtime errors
- Technical debt

**Recommended Fix:**
Enable strict TypeScript checking and fix all `any` usages.

---

### 3.2 No Streaming Cancellation

**Problem:**
Once a stream starts, there's no way to gracefully cancel it:

```typescript
const result = streamText({ ... });
return result.toUIMessageStreamResponse({ headers: corsHeaders });

// No AbortController, no cancellation
```

**Impact:**
- Wasted compute if user navigates away
- Cannot implement "stop generation" button easily

**Recommended Fix:**

```typescript
const abortController = new AbortController();

const result = streamText({
  abortSignal: abortController.signal,
  // ...
});

// Expose abort to frontend via separate endpoint
Deno.serve(async (req) => {
  if (req.url.endsWith('/abort')) {
    abortController.abort();
    return new Response('Aborted', { status: 200 });
  }
  // ...
});
```

---

### 3.3 No Cost Limits Per User

**Problem:**
Usage is tracked but not limited:

```typescript
// Usage is logged but never checked against limits
logUsage(supabase, { userId, model, inputTokens, outputTokens });
```

**Impact:**
- Potential for runaway costs
- No protection against abuse

**Recommended Fix:**

```typescript
async function checkCostLimit(supabase: SupabaseClient, userId: string, estimatedCost: number): Promise<boolean> {
  const { data: usage } = await supabase
    .from('embedding_usage_logs')
    .select('cost_cents')
    .eq('user_id', userId)
    .gte('created_at', new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString()); // 30 days
  
  const totalCost = usage.reduce((sum, row) => sum + row.cost_cents, 0);
  const limitCents = 1000; // $10 limit
  
  return totalCost + estimatedCost <= limitCents;
}
```

---

## 4. Scalability Analysis

### 4.1 Current Capacity Estimates

**Assumptions:**
- Average request: 3 tool calls
- Average tool execution: 800ms
- Edge function timeout: 400 seconds
- Supabase concurrent connection limit: ~100 (depends on plan)

**Estimated Max Concurrent Users:**

| Metric | Estimate |
|--------|----------|
| Requests per user per minute | 5 |
| Average request duration | 8 seconds |
| Concurrent requests per user | 0.67 |
| **Max concurrent users** | **~150** |

**Bottlenecks at Scale:**

1. **Database Connections** (Primary)
   - Each tool call opens DB connection
   - Connection pool exhaustion under load

2. **Embedding API Rate Limits** (Secondary)
   - 3000 RPM limit on OpenAI
   - No caching compounds the problem

3. **Re-ranking Latency** (Tertiary)
   - Serial processing doesn't scale
   - HuggingFace rate limits

### 4.2 Scaling Path to 1000+ Users

**Phase 1: Caching (Immediate - 2 weeks)**
- Add Redis for result caching
- Implement embedding cache
- **Impact:** 5-10x capacity increase

**Phase 2: Connection Pooling (Short-term - 1 month)**
- Use Supabase connection pooling (PgBouncer)
- Optimize query patterns
- **Impact:** 2-3x capacity increase

**Phase 3: Async Processing (Medium-term - 2 months)**
- Move re-ranking to background job
- Parallel tool execution
- **Impact:** 3-5x capacity increase

**Phase 4: Architecture (Long-term - 3+ months)**
- Split into micro-functions
- Queue-based architecture
- Regional deployment
- **Impact:** 10x+ capacity increase

---

## 5. Action Items

### Immediate (This Sprint)

- [ ] **Add query result caching with Redis**
  - File: `supabase/functions/_shared/search-pipeline.ts`
  - Effort: 2 days
  - Impact: HIGH

- [ ] **Implement embedding cache**
  - File: `supabase/functions/_shared/embeddings.ts`
  - Effort: 1 day
  - Impact: MEDIUM

### Short-term (Next Month)

- [ ] **Parallelize re-ranking**
  - File: `supabase/functions/_shared/search-pipeline.ts`
  - Effort: 2 days
  - Impact: HIGH

- [ ] **Add cost limits per user**
  - File: `supabase/functions/_shared/usage-tracker.ts`
  - Effort: 1 day
  - Impact: MEDIUM

### Medium-term (Next Quarter)

- [ ] **Modularize chat-stream-v2**
  - Split into separate files
  - Add unit tests per tool
  - Effort: 3 days
  - Impact: MEDIUM

- [ ] **Move pricing to database**
  - Create `model_pricing` table
  - Add sync job
  - Effort: 2 days
  - Impact: LOW

### Long-term (Future)

- [ ] **Multi-provider embedding fallback**
  - Add local embedding model option
  - Effort: 1 week
  - Impact: MEDIUM

- [ ] **Streaming cancellation support**
  - Add AbortController
  - Frontend stop button
  - Effort: 2 days
  - Impact: LOW

---

*Concerns analysis: 2026-02-09*
